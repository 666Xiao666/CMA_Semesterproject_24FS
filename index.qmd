---
title: Trip detection from daily GPS data
subtitle: a review and framework of generating walking trajectories
author: Xiao Cui
format:
  html:
    code-fold: true
    number-sections: true
    colorlinks: true
execute:
  warning: false
  message: false
lang: en
bibliography: bibliography.bib
editor: 
  markdown: 
    wrap: sentence
---

```{r preprocessing}
#| code-summary: load packages, import data
# check packages: ------------------------------------------------------------------------------
## Default repository
local({r <- getOption("repos")
r["CRAN"] <- "http://cran.r-project.org" 
options(repos=r)
})

check_pkg <- function(x)
{
  if (!require(x, character.only = TRUE, quietly = TRUE))
  {
    install.packages(x, dep = TRUE, verbose = FALSE, quiet = TRUE)
    if(!require(x, character.only = TRUE, quietly = TRUE)) stop("Package not found")
  }
}
check_pkg("dplyr")
check_pkg("lubridate")
check_pkg("geosphere")
check_pkg("tidyverse")
check_pkg("sf") # 
check_pkg("purrr") # package for precise calculation
check_pkg("readr") # a better way to read data than basic R
check_pkg("rnaturalearth") # package for requiring DEM data
check_pkg("raster")
check_pkg("elevatr") # package for handling DEM
check_pkg("terra") # package for raster data handling
check_pkg("skimr") # a better package to summarize data
check_pkg("ggplot2") # package for plots
check_pkg("smplot2") # package for more functions based on ggplot
check_pkg("mapview") # package for interactive maps
check_pkg("randomForest") # package for random forest
check_pkg("caret") # package for random forest
check_pkg("e1071") # package for random forest
# import data ------------------------------------------------------------------------------
posmo_df <- readr::read_csv("posmo_2024-03-23_2024-06-15.csv",
                 col_types = cols(
                   user_id = col_character(),
                   datetime = col_datetime(),
                   weekday = col_factor(levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")), # read daytime as a factor
                   place_name = col_character(),
                   transport_mode = col_factor(levels = c("Walk", "Tram", "Bus", "Bike", "Train", "Car")),
                   lon_x = col_double(),
                   lat_y = col_double()
                 )) |> dplyr::select(-user_id) # remove user_id because we only have one user here
```

## Abstract
Daily GPS trajectories have a huge potential in understanding people's daily mobility patterns. However, there is still uncertainty in how to detect travel modes from abundant GPS point data accurately. We review and summarize recent studies about GPS data reprocessing,  combining geographic context with trajectories, and travel mode detection. Then we detect walking travel mode from daily GPS data generated by POSMO application under this framework. We focus on walking trip identification and rebuild two rule-based algorithms for raw point data and utilize random forest with labeled GPS coordinates.
Result shows random forest has the best performance among three detection methods with the accuracy of 93.8%. However, rule-based algorithms are more understandable and doable with around 85% accuracy with the optimal parameters. Speed is the primary factor for identifying walking travel mode compared with other parameters, but acceleration can enhance the precision in the model. Our study provides a framework of identifying walking trajectories and supplementing them by combining other geospatial contexts and map matching.

## Introduction
With the advancement of technologies such as mobile devices and Internet, Global Positioning System (GPS) trajectory data generated by users, vehicles, and other mobile objects has experienced explosive growth. Massive trajectory data can depict the spatiotemporal movement patterns, which have enormous value in travel strategy @jiaqi2018transport, environmental planning, and policy making @yun2018spatiotemporal. 
extract travel information, and thus analyze travel purposes, preferences in travel modes, and travel distribution @montini2014trip.
However, it is still challenging to detect travel modes from daily GPS data. Daily GPS data contains mixed travel modes @s18113741. @HUANG2019297 summarize how to from complex by mobile data.
Walking is not only a common travel mode in daily life, but also a low-cost physical activity which can contribute to . @cho2011identifying define. @yun2018spatiotemporal use kernel density to generate walking trajectories.
Travel mode detection.
@XIAO201514 identify travel modes by Bayesian network from smartphone-based travel surveys.
@s18113741 review how to detect travel modes from with complex travel modes. Later efforts incorporated machine learning (ML) to derive classification "rules" directly from input features, enhancing decision-making flexibility and robustness against real-world noise. Examples of ML-based techniques include decision trees, random forests (RF), support vector machines, and artificial neural networks. Among these, RF has gained significant popularity due to its capability to manage high feature dimensionality and multicollinearity effectively. Studies comparing various mode detection methods consistently show that RF outperforms other classical ML algorithms in terms of performance @hong_evaluating_2023.
@SADEGHIAN2022159 mentioned random forest performs best among supervised machine learning.
Walking is a low cost and low carbon physical activities, which can benefit carbon emission. We define walking as "". 
In our study, we tend to answer following research questions:
Q1: How can we rebuild (heuristic) rule-based walking detection methods under point-based segmentation in R?
Q2: How do we evaluate the performance of walking detection methods?
Q3: How do we match GPS points to a complex road network by topological method?
Q4: Does supervised classification (random forest) perform better than rule-based (heuristic) method in walking detection? 
Q5: How to combine other semantic contexts on trajectories?

## Material and Methods

### Data

#### Data collection
The smartphone for data collection is iPhone Xs Max with iOS Version 17.5.1. We refer to the documents about data protection under [POSMO application](https://posmo.coop/about-us/datenschutzerklaerung). GPS coordinates here are collected via anonymized IP addresses by removing the last 2 bytes (i.e. 198.51.0.0 instead of 198.51.100.54). 
However, the algorithm here is still ambiguous. We assume POSMO application has similar functions with a GPS logger (i.e., do not record data while stationary). Trip endpoints are typically identified when the dwell time between two consecutive moving data points exceeds a certain threshold. When the GPS loggers resume movement, they require a few seconds to a few minutes to warm start, leading to missing GPS points at the beginning of the trip @gong2012gps. From the repetitive points statistics below, we find there are 804 endpoints in our raw data. The repeated count of endpoints is between 2 to 4, i.e., when POSMO application detects a "stop", it will acquire the place name of current anonymized IP address by geocoding (retrieving place names by coordinates), then store this name in the column `place_name`.

```{r}
#| code-summary: a review of stop detection in POSMO
# Group by datetime and count the number of occurrences
repeated_counts <- posmo_df |> 
  group_by(datetime) |> 
  summarise(count = n()) |> 
  filter(count > 1)

# Display the repeated_counts partly
cat(" Stop points detection:", "\n","Total count of stops:", nrow(repeated_counts), "\n","Possible point count of each stop:", unique(repeated_counts$count), "\n")
# remove the temporary dataframe
rm(repeated_counts)
```
The first step is to distinguish trip and activities from GPS points. 

Here we find when.





```{r raw segment}
#| code-summary: generate raw walking trajectories
# Convert coordinates to LV95 (Swiss coordinate system)
coordinates <- st_as_sf(posmo_df, coords = c("lon_x", "lat_y"), crs = 4326, remove = FALSE)
coordinates_lv95 <- st_transform(coordinates, 2056)
posmo_df$lv95_x <- st_coordinates(coordinates_lv95)[,1]
posmo_df$lv95_y <- st_coordinates(coordinates_lv95)[,2]

# Assign transport_mode_id based on transport_mode
# Filter out rows with null values in the transport_mode column
posmo_df <- posmo_df %>%
  filter(!is.na(transport_mode))

# Assign trip identifiers based on changes in transport_mode
posmo_df <- posmo_df %>%
  mutate(transport_mode_id = cumsum(c(1, as.numeric(transport_mode[-1] != transport_mode[-n()]))))

walk <- posmo_df |> filter(transport_mode == "Walk")

walk_sf <- st_as_sf(walk,
    coords = c("lv95_x", "lv95_y"),
    crs = 2056, remove = FALSE
)

# Create LINESTRING by grouping by transport_mode_id
walk_line <- walk_sf %>% 
  group_by(transport_mode_id) %>% 
  summarise(do_union = FALSE) %>% 
  st_cast("LINESTRING") |> 
  mutate(length = st_length(geometry))

# Group points by transport_mode_id and calculate the time duration for each group
time_durations <- walk_sf %>%
  group_by(transport_mode_id) %>%
  summarise(
    start_time = min(datetime),
    end_time = max(datetime),
    time_duration = as.numeric(difftime(max(datetime), min(datetime)), units = "secs")
  ) |> st_drop_geometry()

# Create LINESTRING by grouping by transport_mode_id
walk_line <- walk_sf %>%
  group_by(transport_mode_id) %>%
  summarise(do_union = FALSE) %>%
  st_cast("LINESTRING") %>%
  mutate(length = st_length(geometry))

# Join the time durations back to walk_line
walk_line <- walk_line %>%
  left_join(time_durations %>% dplyr::select(transport_mode_id, time_duration), by = "transport_mode_id")

# Calculate the mean speed
walk_line <- walk_line %>%
  mutate(speed_m_s = as.numeric(length / time_duration))|>
  mutate()

# Sample the walk_line data
sampled_walk_line <- walk_line %>%
  sample_n(1)

# view the sample by map
mapview(sampled_walk_line, zcol = "transport_mode_id")
```



#### Data attributes
We used daily GPS data collected by POSMO application with the following attributes:

| Variable           | Label                                          |
|--------------------|------------------------------------------------|
| user_id  | the user id of POSMO account                   |
| datetime| timestamp of a GPS point                       |
| weekday | weekday of timestamp (from Sunday to Saturday) |
| place_name | the place name where the user is detected as "stop" |
| **transport_mode** | travel mode detected by POSMO application|
| lon_x | longitude of GPS point                         |
| lat_x | latitude of GPS point                          |

: Table 1: attributes in POSMO GPS data {.striped .hover}

```{r summarize data}
#| code-summary: have a review on the raw data
# have a breif view of our data
# Check structure of dataset
skimr::skim(posmo_df)
head(posmo_df)
tail(posmo_df)
# Extract the date from the datetime column
posmo_df <- posmo_df |> mutate(date = as_date(datetime))
# Count of days with records
length(unique(posmo_df$date))
# date records from
min(posmo_df$date)
# date records from
max(posmo_df$date)
```

GPS data are recorded by 5-15 second intervals and labeled with transform mode. Users can fix this attribute by adjusting transfer mode manually then validate and export data as a csv document. 
There are 37174 GPS points in the raw data set with 78 days (from 2024-03-23 to 2024-06-15).


```{r data cleaning}
#| code-summary: remove data with duplicated timestamps
posmo_df <- distinct(posmo_df,datetime,.keep_all=TRUE)
```

If there is no movement within one day, there will be only one location (`place_name` in the attribute table) recorded. Here we define "valid trip day" as "the day with at least three GPS records". Based on that rule, we generate 37151 GPS points, with 55 valid trip information. Figure 1 below shows the among different dates.
```{r}
# Extract the date from the datetime column
# Exclude the data if the record of the day is less than three
posmo_df <- posmo_df |> mutate(date = as_date(datetime)) |> group_by(date) |>
  filter(n() >= 3) |>
  ungroup()
nrow(posmo_df)
length(unique(posmo_df$date))

# Count the number of data points per date
# Create a new column for the month
date_counts <- posmo_df |> 
  mutate(date = as.Date(datetime),
         month = month(date, label = TRUE)) |> 
  group_by(date, month) |> 
  summarise(count = n(), .groups = 'drop')

# Plot the result with points colored by month
ggplot(date_counts, aes(x = date, y = count, color = month)) +
  geom_line() + 
  geom_point() +
  labs(x = "Date", y = "Count of GPS Points", title = "Figure 1: Count of GPS Points per Date", color = "Month") +
  theme_minimal() 
# remove this temporary dataframe after plotting the result
rm(date_counts)
```

Although POSMO application can identify the stay point as the end of a trip, we still need to validate whether there are drifted points in our data.
### Detection

### Reprocessing
Dbscan is widely applied in detecting noise in GPS data. This method is easily conducted with two parameters: min

#### Heuristic approach

Heuristic trip identification algorithms utilize a set of analyst-designed rules to identify trips within GPS trajectories. These rules establish thresholds that distinguish trip records from activity records by examining data features such as time differences between records, speed, and record density.

For instance, many heuristic algorithms apply low-speed thresholds, typically in the range of 0 to 0.15 m/s, to identify activity records (Dalumpines and Scott, 2017; Rewa, 2012; Schuessler and Axhausen, 2009; Stopher et al., 2008; Tsui and Shalaby, 2006; Wolf et al., 2001).

Another common method involves using thresholds for dwell time or time differences between consecutive records, sometimes in conjunction with a speed threshold, to identify activities. Typically, a threshold of 120 seconds is used, corresponding to traffic signal cycle lengths (Dalumpines and Scott, 2017; Rewa, 2012; Shen and Stopher, 2013; Stopher et al., 2008; Wolf et al., 2001) @berjisian2022evaluation.

The standard procedure for processing GPS data to identify travel modes involves three steps. First, the data collected from GPS-enabled smartphones is transferred to computers, where output files suitable for further statistical analysis are created. Second, trips or segments are identified. Finally, travel modes are detected based on the previously processed data.

The median speed, percentile speed, average speed, steady speed, and average dwell time were selected as the basic features for the travel mode identification algorithm.
The focus was on extracting features related to speed, and the percentile speed was used instead of the maximum speed because the latter might not reflect the actual speed, which could be a sudden and fluctuating value.

### walking trip detection
@cho2011identifying define criteria for identifying outdoor walking trips from GPS data. The threshold here is consecutive track points. We adjust the algorithm slightly to interval in our data (5-15 seconds).
- 1. speed should be more than 2 km/h (0.56 m/s)and less than 8 km/h (2.22 m/s); 
- 2. generate gaps if there are more than five consecutive points;
- 3. divide GPS data into trips by gaps;
- 4. duration is larger than 180 s;
- 5. average speed should be no more than 6 km/h (1.67 m/s).
```{r}

```

The common threshold of walking speed is from 0 to 0.15 m/s. We refer to the walking detection method from @gong2012gps, with the process and rules after adjustment as follow: 
- 1. speed should be more than 1.6 km/h (0.44 m/s)and less than 10 km/h (2.78 m/s) within 1 min interval; 
- 2. gaps if time interval > 120 s and distance > 250 m;
- 3. divide GPS data into trips by gaps;
- 4. average speed should be no more than 6 km/h (1.67 m/s).

```{r}
# Define helper functions
distance_by_element <- function(later, now) {
    as.numeric(st_distance(later, now, by_element = TRUE))
  }

difftime_secs <- function(later, now){
    as.numeric(difftime(later, now, units = "secs"))
  }

posmo_sf <- posmo_df |>
    st_as_sf(coords = c("lon_x", "lat_y"), crs = 4326, remove = FALSE) |> 
    st_transform(2056)

# rebuilding algorithm
posmo_sf_gong <- posmo_sf |>
      mutate(
        stepMean = distance_by_element(geometry, lag(geometry, 6))
      ) |>
      mutate(
        timeMean = difftime_secs(datetime, lag(datetime, 6))
      ) |> 
      mutate(speedMean = stepMean / timeMean) |>
      mutate(gap = ifelse(stepMean > 250 | timeMean > 150, TRUE, FALSE))


# Assign trip IDs based on gaps
posmo_sf_gong <- posmo_sf_gong |> 
  mutate(tra_id = cumsum(gap) + 1)

# Assign initial trip IDs based on gaps
posmo_sf_gong <- posmo_sf_gong |> 
  mutate(tra_id = cumsum(gap) + 1)

# Function to check and adjust trip IDs based on average speed
adjust_trip_ids <- function(df) {
  df <- df |> arrange(datetime)
  df <- df |> group_by(tra_id) |> 
    mutate(avg_speed = mean(speedMean, na.rm = TRUE)) |> 
    ungroup()
  
  df <- df |> 
    mutate(
      tra_id = cumsum(lag(cumsum(gap), default = 0)) + 1,
      new_gap = ifelse(avg_speed > 1.67, 1, 0)
    ) |> 
    mutate(tra_id = cumsum(new_gap) + 1) |> 
    select(-avg_speed, -new_gap)
  
  return(df)
}

# Apply the adjustment function
posmo_sf_gong <- adjust_trip_ids(posmo_sf_gong)
```

#### Sensitivity analysis
```{r speed based detection}
#| code-summary: speed based detection
# Define the function to compute performance metrics for different lag values
compute_metrics_for_lag <- function(posmo_df, lag_values) {
  
  # Define helper functions
  distance_by_element <- function(later, now) {
    as.numeric(st_distance(later, now, by_element = TRUE))
  }

  difftime_secs <- function(later, now){
    as.numeric(difftime(later, now, units = "secs"))
  }
  
  # Convert dataframe to sf object
  posmo_sf <- posmo_df |>
    st_as_sf(coords = c("lon_x", "lat_y"), crs = 4326, remove = FALSE) |> 
    st_transform(2056)
  
  # Initialize results list
  results <- list()
  
  # Iterate over each lag value
  for (lag_value in lag_values) {
    
    posmo_sf <- posmo_sf |>
      mutate(
        stepMean = distance_by_element(geometry, lag(geometry, lag_value))
      ) |>
      mutate(
        timeMean = difftime_secs(datetime, lag(datetime, lag_value))
      ) |> 
      mutate (speedMean = stepMean / timeMean) |> 
      mutate (acc = (lead(speedMean) - speedMean) / timeMean) |>
      mutate(
        walk = (speedMean <= 1.7)
      ) |>
      mutate(
        walk_s_a = (speedMean <= 1.7) & (abs(acc) <= 0.009)
      )
    
    # Validation of speed and acceleration based detection
    walk_tp <- posmo_sf |> filter(transport_mode == "Walk" & walk == TRUE)
    walk_fp <- posmo_sf |> filter(transport_mode != "Walk" & walk == TRUE)
    walk_fn <- posmo_sf |> filter(transport_mode == "Walk" & walk != TRUE)
    walk_tn <- posmo_sf |> filter(transport_mode != "Walk" & walk != TRUE)
    
    tp <- nrow(walk_tp)
    fp <- nrow(walk_fp)
    fn <- nrow(walk_fn)
    tn <- nrow(walk_tn)
    
    accuracy <- (tp + tn) / (tp + tn + fp + fn)
    precision <- tp/ (tp + fp)
    specificity <- tn / (tn + fp)
    sensitivity <- tp / (tp + fn)
    recall <- tp / (tp + fn)
    balance_accuracy <- (specificity + sensitivity) / 2
    f1 <- 2 * (precision * recall) / (precision + recall)
    
    # Store results for the current lag value
    results[[as.character(lag_value)]] <- c(
      accuracy = accuracy,
      precision = precision,
      specificity = specificity,
      sensitivity = sensitivity,
      recall = recall,
      balance_accuracy = balance_accuracy,
      f1_score = f1
    )
  }
  
  # Convert results list to data frame
  results_df <- do.call(rbind, results)
  return(results_df)
}

# Values here represent minutes
lag_values <- c(1, 2, 3, 4, 5)

# Compute metrics for the given lag values
metrics_df <- compute_metrics_for_lag(posmo_df, lag_values)

# Display the results
print(metrics_df)
```

```{r speed-acceleration based detection}
#| code-summary: speed-acceleration based detection
# Define the function to compute performance metrics for different lag values
compute_metrics_for_lag <- function(posmo_df, lag_values) {
  
  # Define helper functions
  distance_by_element <- function(later, now) {
    as.numeric(st_distance(later, now, by_element = TRUE))
  }

  difftime_secs <- function(later, now){
    as.numeric(difftime(later, now, units = "secs"))
  }
  
  # Convert dataframe to sf object
  posmo_sf <- posmo_df |>
    st_as_sf(coords = c("lon_x", "lat_y"), crs = 4326, remove = FALSE) |> 
    st_transform(2056)
  
  # Initialize results list
  results <- list()
  
  # Iterate over each lag value
  for (lag_value in lag_values) {
    
    posmo_sf <- posmo_sf |>
      mutate(
        stepMean = distance_by_element(geometry, lag(geometry, lag_value))
      ) |>
      mutate(
        timeMean = difftime_secs(datetime, lag(datetime, lag_value))
      ) |> 
      mutate (speedMean = stepMean / timeMean) |> 
      mutate (acc = (lead(speedMean) - speedMean) / timeMean) |>
      mutate(
        walk = (speedMean <= 1.7)
      ) |>
      mutate(
        walk_s_a = (speedMean <= 1.7) & (abs(acc) <= 0.009)
      )
    
    # Validation of speed and acceleration based detection
    walk_tp_s_a <- posmo_sf |> filter(transport_mode == "Walk" & walk_s_a == TRUE)
    walk_fp_s_a <- posmo_sf |> filter(transport_mode != "Walk" & walk_s_a == TRUE)
    walk_fn_s_a <- posmo_sf |> filter(transport_mode == "Walk" & walk_s_a != TRUE)
    walk_tn_s_a <- posmo_sf |> filter(transport_mode != "Walk" & walk_s_a != TRUE)
    
    tp_s_a <- nrow(walk_tp_s_a)
    fp_s_a <- nrow(walk_fp_s_a)
    fn_s_a <- nrow(walk_fn_s_a)
    tn_s_a <- nrow(walk_tn_s_a)
    
    accuracy_s_a <- (tp_s_a + tn_s_a) / (tp_s_a + tn_s_a + fp_s_a + fn_s_a)
    precision_s_a <- tp_s_a / (tp_s_a + fp_s_a)
    specificity_s_a <- tn_s_a / (tn_s_a + fp_s_a)
    sensitivity_s_a <- tp_s_a / (tp_s_a + fn_s_a)
    recall_s_a <- tp_s_a / (tp_s_a + fn_s_a)
    balance_accuracy_s_a <- (specificity_s_a + sensitivity_s_a) / 2
    f1_s_a <- 2 * (precision_s_a * recall_s_a) / (precision_s_a + recall_s_a)
    
    # Store results for the current lag value
    results[[as.character(lag_value)]] <- c(
      accuracy = accuracy_s_a,
      precision = precision_s_a,
      specificity = specificity_s_a,
      sensitivity = sensitivity_s_a,
      recall = recall_s_a,
      balance_accuracy = balance_accuracy_s_a,
      f1_score = f1_s_a
    )
  }
  
  # Convert results list to data frame
  results_df <- do.call(rbind, results)
  return(results_df)
}

# Example usage
lag_values <- c(1, 2, 3, 4, 5)

# Compute metrics for the given lag values
metrics_df <- compute_metrics_for_lag(posmo_df, lag_values)

# Display the results
print(metrics_df)
```

```{r threshold test}
#| code-summary: define the function to compute performance metrics for different speedMean and abs(acc) threshold values

compute_metrics_for_thresholds <- function(posmo_df, thresholds) {
  
  # Define helper functions
  distance_by_element <- function(later, now) {
    as.numeric(st_distance(later, now, by_element = TRUE))
  }

  difftime_secs <- function(later, now){
    as.numeric(difftime(later, now, units = "secs"))
  }
  
  # Convert dataframe to sf object
  posmo_sf <- posmo_df |>
    st_as_sf(coords = c("lon_x", "lat_y"), crs = 4326, remove = FALSE) |> 
    st_transform(2056)
  
  # Initialize results list
  results <- list()
  
  # Iterate over each pair of thresholds
  for (threshold in thresholds) {
    speed_threshold <- threshold[1]
    acc_threshold <- threshold[2]
    
    posmo_sf <- posmo_sf |>
      mutate(
        stepMean = distance_by_element(geometry, lag(geometry, 6))  # Keeping lag value fixed as 6
      ) |>
      mutate(
        timeMean = difftime_secs(datetime, lag(datetime, 6))
      ) |> 
      mutate (speedMean = stepMean / timeMean) |> 
      mutate (acc = (lead(speedMean) - speedMean) / timeMean) |>
      mutate(
        walk = (speedMean <= speed_threshold)
      ) |>
      mutate(
        walk_s_a = (speedMean <= speed_threshold) & (abs(acc) <= acc_threshold)
      )
    
    # Validation of speed and acceleration based detection
    walk_tp_s_a <- posmo_sf |> filter(transport_mode == "Walk" & walk_s_a == TRUE)
    walk_fp_s_a <- posmo_sf |> filter(transport_mode != "Walk" & walk_s_a == TRUE)
    walk_fn_s_a <- posmo_sf |> filter(transport_mode == "Walk" & walk_s_a != TRUE)
    walk_tn_s_a <- posmo_sf |> filter(transport_mode != "Walk" & walk_s_a != TRUE)
    
    tp_s_a <- nrow(walk_tp_s_a)
    fp_s_a <- nrow(walk_fp_s_a)
    fn_s_a <- nrow(walk_fn_s_a)
    tn_s_a <- nrow(walk_tn_s_a)
    
    accuracy_s_a <- (tp_s_a + tn_s_a) / (tp_s_a + tn_s_a + fp_s_a + fn_s_a)
    precision_s_a <- tp_s_a / (tp_s_a + fp_s_a)
    specificity_s_a <- tn_s_a / (tn_s_a + fp_s_a)
    sensitivity_s_a <- tp_s_a / (tp_s_a + fn_s_a)
    recall_s_a <- tp_s_a / (tp_s_a + fn_s_a)
    balance_accuracy_s_a <- (specificity_s_a + sensitivity_s_a) / 2
    f1_s_a <- 2 * (precision_s_a * recall_s_a) / (precision_s_a + recall_s_a)
    
    # Create a row name based on the current thresholds
    row_name <- paste0("speed=", speed_threshold, ",acc=", acc_threshold)
    
    # Store results for the current pair of thresholds
    results[[row_name]] <- c(
      accuracy = accuracy_s_a,
      precision = precision_s_a,
      specificity = specificity_s_a,
      sensitivity = sensitivity_s_a,
      recall = recall_s_a,
      balance_accuracy = balance_accuracy_s_a,
      f1_score = f1_s_a
    )
  }
  
  # Convert results list to data frame
  results_df <- do.call(rbind, results)
  return(results_df)
}

# Example usage
thresholds <- list(
  c(1.5, 0.001),
  c(1.7, 0.001),
  c(1.9, 0.001),
  c(1.5, 0.002),
  c(1.7, 0.002),
  c(1.9, 0.002)
)

# Compute metrics for the given thresholds
metrics_df <- compute_metrics_for_thresholds(posmo_df, thresholds)

# Display the results
print(metrics_df)
```

We change the threshold of speed and acceleration to examine their effects on walking detection.

### Random forest
The Random Forest algorithm is an improvement over decision trees and consists of multiple decision trees @Roy2020Classifying. For an individual tree, nodes are created by randomly selecting data and features during the training model. The node value is determined by the threshold that best splits the data, i.e., the situation where a particular label of travel mode accounts for the highest proportion of the segmented data. The growth stops when the label type becomes unique. 

During model recognition, the test data is compared with the node values and falls into a specific sub-node accordingly.

This comparison of features is repetitive until the data falls into a leaf node. The label of the leaf node represents the recognition result of the trajectory data segment. All the decision trees' recognition results are then put to a vote, and the travel mode label with the highest votes is determined to be the identification result of the Random Forest.

The essence of the Random Forest algorithm lies in its use of a dichotomous method based on thresholds, which is more suited to solving classification problems. Therefore, we convert the problem of identifying travel modes from a segment of trajectory data into a classification problem, utilizing the random Forest algorithm to complete the travel mode identification.

```{r}
# Check rows where datetime is out of order
disordered_df <- posmo_df %>%
  mutate(is_ordered = c(TRUE, diff(datetime) >= 0)) %>%
  filter(is_ordered = FALSE) |> as_tibble()
```

Here we find there is no systematic error about temporal disorder in our data, potentially because we use application based GPS data here (i.e., such errors have been fixed before exportation).

### Spatial context

@hong_evaluating_2023 combine labeled GPS trajectories with geospatial context features (road network, water, green space, residential areas) for a preciser travel mode detection by machine learning. Here we utilize walkway network and digital elevation model to (1) fix the walking trips; (2) examine the correlation between gradients (slope) and walking speed.

### Map matching
Raw GPS coordinates complexity of urban road network @HE2019122318. The objective is to accurately align GPS points collected during walks with the corresponding walkways on a map, ensuring spatial accuracy and continuity.
We match `walk_sf` (point features containing GPS points collected during walks) with `walkway_sf` (a geoPackage file containing the linestrings), representing the walkways in Zurich. Both simple features are projected in the CH1903+ / LV95 CRS. A 10-meter buffer is created around each GPS point to identify intersecting line strings within a reasonable proximity (i.e., the width of walking streets in reality). This buffer helps in reducing the search space for potential matches. 
Linestrings that intersect with each buffer are identified by spatial joining. This step narrows down the linestrings that are potential matches for each GPS point. For each GPS point, the nearest intersecting linestring is determined based on the shortest distance. The snapping function ensures that each point is aligned to the closest point on the nearest linestring. Finally, the snapped points are converted into a single geometry column and integrated into a new sf object.
```{r matching}
#| code summary: map matching with sample
# Load the walkway network
walkway_sf <- st_read("foot_zurich.gpkg", layer = "taz_mm.tbl_routennetz")

# Convert walk points to sf object
walk_sf <- st_as_sf(walk, coords = c("lv95_x", "lv95_y"), crs = 2056, remove = FALSE)

# Ensure both sf objects have the same CRS
walkway_sf <- st_transform(walkway_sf, crs = st_crs(walk_sf))

# Create a buffer around each point
buffered_points <- st_buffer(walk_sf, dist = 10)

# Find intersecting lines for each point
intersections <- st_intersects(buffered_points, walkway_sf)

# Function to find the nearest linestring for each point within the buffer
find_nearest_line <- function(point, lines) {
  # Ensure CRS match
  if (st_crs(point) != st_crs(lines)) {
    lines <- st_transform(lines, crs = st_crs(point))
  }
  
  if (nrow(lines) == 0) {
    return(st_geometry(point))
  }
  
  distances <- st_distance(point, lines)
  nearest_line_idx <- which.min(distances)
  nearest_line <- lines[nearest_line_idx, ]
  
  nearest_point <- st_nearest_points(point, nearest_line)
  st_cast(nearest_point, "POINT")[2]  # Take the second point which is on the linestring
}

# Apply the function to each point in walk_sf
walk_snapped <- walk_sf %>%
  rowwise() %>%
  mutate(
    snapped_geometry = list(find_nearest_line(geometry, walkway_sf[intersections[[cur_group_id()]], ]))
  ) %>%
  ungroup()

# Convert snapped_geometry to a single geometry column
snapped_geometries <- st_sfc(do.call(c, lapply(walk_snapped$snapped_geometry, st_geometry)), crs = st_crs(walk_sf))

# Create a new sf object with the snapped geometries
walk_snapped_sf <- walk_snapped %>%
  mutate(geometry = snapped_geometries) %>%
  st_as_sf() %>%
  dplyr::select(-snapped_geometry)

# Create LINESTRING by grouping by transport_mode_id
walk_snapped_line <- walk_snapped_sf %>% 
  group_by(transport_mode_id) %>% 
  summarise(do_union = FALSE) %>% 
  st_cast("LINESTRING")

# Define the number of lines to sample
number_of_lines_to_sample <- 1

# Sample lines with the same transport_mode_id from both collections
sampled_walk_line <- walk_line %>%
  group_by(transport_mode_id) %>%
  sample_n(number_of_lines_to_sample) %>%
  ungroup() %>%
  mutate(source = "original")

sampled_walk_snapped_line <- walk_snapped_line %>%
  group_by(transport_mode_id) %>%
  sample_n(number_of_lines_to_sample) %>%
  ungroup() %>%
  mutate(source = "snapped")

# Combine the sampled lines into a single sf object
combined_lines <- bind_rows(sampled_walk_line, sampled_walk_snapped_line)
#
sample_combine <- combined_lines |> filter(transport_mode_id == sample(1:262, 1, replace = TRUE))
# Plot the lines on the same map with different colors
mapview(sample_combine, zcol = "source")
```







## Results





We apply the algorithm @gong2012gps to detect walking travel.

```{r random forest}
#| code summary: random forest with parameters
posmo_simple <- posmo_df %>%
  mutate(transport_mode = case_when(
    transport_mode == "Tram" ~ "Vehicle",
    transport_mode == "Train" ~ "Vehicle",
    transport_mode == "Bus" ~ "Vehicle",
    transport_mode == "Car" ~ "Vehicle",
    transport_mode == "Bike" ~ "Walk",
    TRUE ~ transport_mode  # Keep all other values unchanged
  ))

trajectories_all_sf <- st_as_sf(posmo_simple,
    coords = c("lv95_x", "lv95_y"),
    crs = 2056, remove = FALSE
)

# Group points by transport_mode_id and get the unique transport_mode for each group
mode_mapping <- posmo_simple %>%
  group_by(transport_mode_id) %>%
  summarise(transport_mode = first(transport_mode)) %>%
  ungroup()

# Create LINESTRING by grouping by transport_mode_id
trajectories_all_line <- trajectories_all_sf %>% 
  group_by(transport_mode_id) %>% 
  summarise(do_union = FALSE) %>% 
  st_cast("LINESTRING") |> 
  mutate(length = st_length(geometry))

# Join transport_mode to trajectories_all_line
trajectories_all_line <- trajectories_all_line %>%
  left_join(mode_mapping, by = "transport_mode_id") %>%
  mutate(transport_mode = as.factor(transport_mode))

# Group points by transport_mode_id and calculate the time duration for each group
time_duration_all <- trajectories_all_sf %>%
  group_by(transport_mode_id) %>%
  summarise(
    start_time = min(datetime),
    end_time = max(datetime),
    time_duration = as.numeric(difftime(max(datetime), min(datetime)), units = "secs")
  ) |> st_drop_geometry()

# Join the time durations back to walk_line
trajectories_all_line <- trajectories_all_line %>%
  left_join(time_duration_all %>% dplyr::select(transport_mode_id, time_duration), by = "transport_mode_id")

# Calculate the mean speed
trajectories_all_line <- trajectories_all_line %>%
  mutate(speed_m_s = as.numeric(length / time_duration)) |>   mutate(acc_m2_s = as.numeric(speed_m_s / time_duration))

# Random Forest Model
# Remove rows with NA values
trajectories_all_line <- trajectories_all_line %>%
  drop_na()

# Split the data into training and testing datasets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(trajectories_all_line$transport_mode, p = 0.7, list = FALSE)
train_set <- trajectories_all_line[train_index, ]
test_set <- trajectories_all_line[-train_index, ]


# Define the control parameters for cross-validation
trControl <- trainControl(method = "cv", number = 10, search = "grid")

# Run the model with default settings
set.seed(123)
rf_default <- train(transport_mode ~ length + time_duration + speed_m_s + acc_m2_s, 
                    data = train_set,
                    method = "rf",
                    metric = "Accuracy",
                    trControl = trControl)

# Print the results
print(rf_default)

# Tune mtry
set.seed(123)
tuneGrid_mtry <- expand.grid(.mtry = c(1:10))
rf_mtry <- train(transport_mode ~ length + time_duration + speed_m_s + acc_m2_s, 
                 data = train_set,
                 method = "rf",
                 metric = "Accuracy",
                 tuneGrid = tuneGrid_mtry,
                 trControl = trControl)

# Print the results
print(rf_mtry)
best_mtry <- 1
print(best_mtry)

# Define a custom RF method that can handle maxnodes
customRF <- list(type = "Classification",
                 library = "randomForest",
                 loop = NULL,
                 parameters = data.frame(parameter = c("mtry", "maxnodes"),
                                         class = rep("numeric", 2),
                                         label = c("mtry", "maxnodes")),
                 grid = function(x, y, len = NULL, search = "grid") {
                   expand.grid(mtry = c(1:len), maxnodes = c(5:15))
                 },
                 fit = function(x, y, wts, param, lev, last, classProbs, ...) {
                   randomForest(x, y, mtry = param$mtry, maxnodes = param$maxnodes, ...)
                 },
                 predict = function(modelFit, newdata, submodels = NULL) {
                   predict(modelFit, newdata)
                 },
                 prob = function(modelFit, newdata, submodels = NULL) {
                   predict(modelFit, newdata, type = "prob")
                 },
                 levels = function(x) x$classes)

# Tune maxnodes
store_maxnode <- list()
tuneGrid_maxnode <- expand.grid(mtry = best_mtry, maxnodes = c(5:15))

for (maxnodes in c(5:15)) {
  set.seed(123)
  rf_maxnode <- train(transport_mode ~ length + time_duration + speed_m_s + acc_m2_s, 
                      data = train_set,
                      method = customRF,
                      metric = "Accuracy",
                      tuneGrid = tuneGrid_maxnode,
                      trControl = trControl,
                      nodesize = 14,
                      importance = TRUE,
                      ntree = 100)
  
  key <- toString(maxnodes)
  store_maxnode[[key]] <- rf_maxnode
}

# Extract results
results_maxnode <- resamples(store_maxnode)
summary(results_maxnode)

# Find the best maxnodes
best_maxnodes <- which.max(sapply(store_maxnode, function(x) x$results$Accuracy))

# Tune ntrees
store_ntree <- list()

for (ntrees in c(100, 200, 300, 400, 500)) {
  set.seed(123)
  rf_ntree <- train(transport_mode ~ length + time_duration + speed_m_s + acc_m2_s, 
                    data = train_set,
                    method = customRF,
                    metric = "Accuracy",
                    tuneGrid = expand.grid(mtry = best_mtry, maxnodes = best_maxnodes),
                    trControl = trControl,
                    nodesize = 14,
                    importance = TRUE,
                    ntree = ntrees)
  
  key <- toString(ntrees)
  store_ntree[[key]] <- rf_ntree
}

# Extract results
results_ntree <- resamples(store_ntree)
summary(results_ntree)

# Find the best ntrees
best_ntrees <- which.max(sapply(store_ntree, function(x) x$results$Accuracy))

# Evaluate the final model on the test set
best_rf_model <- store_ntree[[best_ntrees]]$finalModel
plot(best_rf_model)
# Ensure the test set has the same columns and format as the training set
test_set_matrix <- model.matrix(transport_mode ~ length + time_duration + speed_m_s + acc_m2_s, data = test_set)

# Predict on the test set
test_predictions <- predict(best_rf_model, newdata = test_set_matrix)

# Evaluate model performance
confusionMatrix(test_predictions, test_set$transport_mode)

# Variable Importance Plot
varImpPlot(best_rf_model)
```


Data Loading and Cleaning
The dataset containing GPS points (posmo_simple) and trajectory information (trajectories_all_sf) was loaded. Each GPS point was associated with a specific transport mode (transport_mode). Additional features were engineered, including the length of trajectories, duration of trips, mean speed, and acceleration. Points were grouped by transport_mode_id, and trajectories were converted into LINESTRING geometries to facilitate analysis.

Cross-Validation and Initial Model
A 10-fold cross-validation setup was used to ensure robust model training and evaluation. Initially, a Random Forest model was trained with default parameters to establish a baseline for comparison. This preliminary model provided insight into the classification capabilities and highlighted areas for further refinement.

Hyperparameter Tuning
To optimize the model's performance, several hyperparameters were tuned:

mtry: The number of variables randomly sampled as candidates at each split was adjusted.
maxnodes: The maximum number of terminal nodes was fine-tuned using a custom Random Forest function.
ntree: The number of trees in the forest was also optimized.
Using the optimal parameters identified during this tuning process, the final Random Forest model was trained, resulting in improved classification performance.

Model Evaluation
The model was evaluated on the test set, and the results were summarized using a confusion matrix and associated statistics. The confusion matrix provided a detailed breakdown of the model's predictions:

True Positives (Vehicle): 45 instances
False Positives (Vehicle): 5 instances
True Negatives (Walk): 76 instances
False Negatives (Walk): 3 instances
The model achieved an accuracy of 93.8%, indicating that it correctly classified 93.8% of the instances. The 95% confidence interval for accuracy ranged from 88.15% to 97.28%, demonstrating high reliability. The No Information Rate (NIR) was 62.79%, significantly lower than the model's accuracy, highlighting its effectiveness. The p-value for accuracy greater than NIR was less than 2e-16, indicating statistical significance.

Other performance metrics included a Kappa value of 0.8684, reflecting strong agreement between predicted and actual classifications, and a McNemar's Test p-value of 0.7237, suggesting no significant difference in marginal proportions. The sensitivity (recall) was 93.75%, and the specificity was 93.83%. The Positive Predictive Value (PPV) was 90.00%, and the Negative Predictive Value (NPV) was 96.20%. The model exhibited balanced performance, with a balanced accuracy of 93.79%.

Variable Importance
The Random Forest model's variable importance plot revealed the following insights:

Mean Decrease in Accuracy: This metric indicated how much the model's accuracy decreased when each variable was excluded. The most influential features were:

speed_m_s: Speed in meters per second
length: Length of the trajectory
acc_m2_s: Acceleration
time_duration: Duration of the trip
Mean Decrease in Gini: This metric showed the total decrease in node impurity (Gini impurity) when a variable was used for splitting. The ranking was similar to the Mean Decrease in Accuracy, with speed_m_s and length being the most critical features, followed by acc_m2_s and time_duration.

These plots indicated that speed_m_s and length were the most crucial features for classifying the transport mode, while acc_m2_s and time_duration also contributed but to a lesser extent.



The Random Forest model shows strong performance in classifying transport modes between "Vehicle" and "Walk". The high accuracy, Kappa, sensitivity, and specificity values indicate that the model is robust and reliable for this classification task. The variable importance plots highlight that speed and length (distance) are the most influential features, followed by acceleration and trip duration.



```{r dem, eval = FALSE}
#| code-summary: require DEM (example, not run)
# get the boundary of Switzerland
map <- ne_countries(type = "countries", country = "Switzerland",scale = "medium", returnclass = "sf")
# get elevation, zoom here means 30 meter resolution
d <- get_elev_raster(locations = map, z = 12, clip = "locations")

# Save the raster DEM to a TIFF file
writeRaster(d, filename = "switzerland_dem_30m.tif", format = "GTiff", overwrite = TRUE)

posmo_sf <- st_as_sf(posmo_df, coords = c("lon_x", "lat_y"), crs = 4326, remove = FALSE)

# Read the TIFF file back later
dem_raster <- raster::raster("switzerland_dem_30m.tif")
```


```{r read DEM}
posmo_sf <- st_as_sf(posmo_df, coords = c("lon_x", "lat_y"), crs = 4326, remove = FALSE)
  

# Read the TIFF file back later
dem_raster <- raster::raster("switzerland_dem_30m.tif")


dem_df <- terra::extract(dem_raster, st_coordinates(posmo_sf))

walk_trip_dem <- cbind(posmo_sf, dem_df)
  
  
# Convert coordinates to LV95 (Swiss coordinate system)
coordinates <- st_as_sf(walk_trip_dem, coords = c("lon_x", "lat_y"), crs = 4326)
coordinates_lv95 <- st_transform(coordinates, 2056)
walk_trip_dem$lv95_x <- st_coordinates(coordinates_lv95)[,1]
walk_trip_dem$lv95_y <- st_coordinates(coordinates_lv95)[,2]

# Calculate distances and speeds between consecutive points
distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
}

difftime_secs <- function(later, now){
    as.numeric(difftime(later, now, units = "secs"))
}

diff_ele <- function(later, now){
    as.numeric(difftime(later, now, units = "secs"))
}

walk_trip_dem <- walk_trip_dem |>
  mutate(
    stepMean = 
        distance_by_element(geometry, lag(geometry, 6))
  ) |>
  mutate(
    timeMean = 
        difftime_secs(datetime, lag(datetime, 6))
  ) |> 
  mutate (speedMean = stepMean/timeMean) |> 
  mutate (acc = (speedMean - lag(speedMean, 6))/timeMean)|> 
  mutate (gradient = (dem_df - lag(dem_df, 6))/stepMean)


walk_trip_dem_filterd <- walk_trip_dem |> filter(transport_mode == "Walk")

cor.test(walk_trip_dem_filterd$speedMean, walk_trip_dem_filterd$gradient, method = "spearman", exact = F)

ggplot(data = walk_trip_dem_filterd, mapping = aes(x = gradient, y = speedMean)) +
  geom_point(shape = 21, fill = "#0f993d", color = "white", size = 3) +
  smplot2::sm_statCorr(corr_method = "spearman")

```

## Discussion

We argue speed and acceleration are two main factors of walking detection, which accords with the conclusion from @berjisian2022evaluation. The reason is that walking is a typical slow-speed physical activity with a huge variation with vehicle in average speed and acceleration.

We compare the characters of rule-based and random forest detection methods. First, 

Our study also has several limitations. Our raw GPS data lack an attribute in signal accuracy, which is a primary factor in evaluating data quality and detecting outdoor trips @muller2022analyzing. We only utilized GPS data generated from one participant, so our framework of walking detection could not apply for big data contributed by a mixed group.

We simplify potential travel modes into two types (i.e, walking and vehicle), so our framework is not applicable for cycling and transit (i.e., tram, train, and bus etc.) detection. We do not resample data set because the counts of walking and vehicle trajectories are close, but in that case our workflow does not apply to an unbalanced dataset.
For map matching, we only consider geometrical relations (i.e., points to segments) and ignore temporal magnitude @HE2019122318. The complexity of our algorithm here is not optimal and we also lack quantitative validation for map matching, which needs more explorations in the future.

## Conclusion
We conclude our study by answering following research questions:

Q1: How can we rebuild (heuristic) rule-based walking detection methods under point-based segmentation in R?

Q2: How do we evaluate the performance of walking detection methods?

Q3: How do we match GPS points to a complex road network by topological method?

Q4: Does supervised classification (random forest) perform better than rule-based (heuristic) method in walking detection? 

Q5: How to combine other semantic contexts on trajectories?

Our study provide a reproducible framework of detecting walking trips from daily GPS data generated by smartphone application. This procedure can conduce to an accurate walking identification and a better understanding of pedestrian behavior.

### Wordcount

<!-- after installing the wordcountadding, remove the line "#| eval: false" -->

```{r word count}
wordcountaddin::text_stats("index.qmd")
```
